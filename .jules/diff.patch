diff --git a/Dockerfile.ingester b/Dockerfile.doc_processor
similarity index 62%
rename from Dockerfile.ingester
rename to Dockerfile.doc_processor
index da5b7ae..b9d258b 100644
--- a/Dockerfile.ingester
+++ b/Dockerfile.doc_processor
@@ -2,7 +2,7 @@ FROM python:3.12-slim
 
 WORKDIR /app
 
-# Install system dependencies required for PDF processing
+# Install system dependencies required for PDF processing by docling
 RUN apt-get update && apt-get install -y --no-install-recommends \
     libgl1 \
     libglib2.0-0 \
@@ -11,8 +11,6 @@ RUN apt-get update && apt-get install -y --no-install-recommends \
 COPY requirements.txt .
 RUN pip install --no-cache-dir -r requirements.txt
 
-COPY ingester.py .
-COPY manage_index.py .
-COPY tests ./tests
+COPY doc_processor.py .
 
-CMD ["python", "ingester.py"]
+CMD ["python", "doc_processor.py"]
diff --git a/Dockerfile.fastmcp b/Dockerfile.fastmcp
new file mode 100644
index 0000000..86ab715
--- /dev/null
+++ b/Dockerfile.fastmcp
@@ -0,0 +1,10 @@
+FROM python:3.12-slim
+
+WORKDIR /app
+
+COPY requirements.txt .
+RUN pip install --no-cache-dir -r requirements.txt
+
+COPY ./fastmcp /app/fastmcp
+
+CMD ["uvicorn", "fastmcp.main:app", "--host", "0.0.0.0", "--port", "8000"]
diff --git a/Dockerfile.file_watcher b/Dockerfile.file_watcher
new file mode 100644
index 0000000..85f21e6
--- /dev/null
+++ b/Dockerfile.file_watcher
@@ -0,0 +1,10 @@
+FROM python:3.12-slim
+
+WORKDIR /app
+
+COPY requirements.txt .
+RUN pip install --no-cache-dir -r requirements.txt
+
+COPY file_watcher.py .
+
+CMD ["python", "file_watcher.py"]
diff --git a/Dockerfile.meili_ingester b/Dockerfile.meili_ingester
new file mode 100644
index 0000000..803a411
--- /dev/null
+++ b/Dockerfile.meili_ingester
@@ -0,0 +1,10 @@
+FROM python:3.12-slim
+
+WORKDIR /app
+
+COPY requirements.txt .
+RUN pip install --no-cache-dir -r requirements.txt
+
+COPY meili_ingester.py .
+
+CMD ["python", "meili_ingester.py"]
diff --git a/doc_processor.py b/doc_processor.py
new file mode 100644
index 0000000..710b4c3
--- /dev/null
+++ b/doc_processor.py
@@ -0,0 +1,133 @@
+
+import os
+import json
+import time
+import logging
+import pika
+from pika.exceptions import AMQPConnectionError
+from pathlib import Path
+from docling.document_converter import DocumentConverter
+
+# Logging configuration
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+
+# Environment variables
+RABBITMQ_HOST = os.getenv('RABBITMQ_HOST', 'rabbitmq')
+RABBITMQ_USER = os.getenv('RABBITMQ_USER', 'user')
+RABBITMQ_PASS = os.getenv('RABBITMQ_PASS', 'password')
+CONSUME_QUEUE = os.getenv('CONSUME_QUEUE', 'file_events')
+PUBLISH_QUEUE = os.getenv('PUBLISH_QUEUE', 'processed_docs')
+
+def connect_to_rabbitmq():
+    """Establish a connection to RabbitMQ, with retries."""
+    credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASS)
+    parameters = pika.ConnectionParameters(RABBITMQ_HOST, credentials=credentials)
+    while True:
+        try:
+            connection = pika.BlockingConnection(parameters)
+            logging.info("Successfully connected to RabbitMQ.")
+            return connection
+        except AMQPConnectionError as e:
+            logging.error(f"Failed to connect to RabbitMQ: {e}. Retrying in 5 seconds...")
+            time.sleep(5)
+
+class DocumentProcessor:
+    def __init__(self):
+        self.converter = DocumentConverter()
+
+    def process_file(self, file_path_str):
+        """Processes a file based on its extension."""
+        try:
+            file_path = Path(file_path_str)
+            if not file_path.exists():
+                logging.error(f"File not found: {file_path_str}")
+                return None
+
+            docs = []
+            if file_path.suffix.lower() == '.pdf':
+                result = self.converter.convert(file_path_str)
+                markdown = result.document.export_to_markdown()
+                doc = {
+                    "id": file_path.stem,
+                    "content": markdown,
+                    "type": "pdf",
+                    "source": file_path.name,
+                    "metadata": {
+                        "format": "markdown",
+                        "page_count": len(result.document.pages) if hasattr(result.document, 'pages') else 0
+                    }
+                }
+                docs.append(doc)
+                logging.info(f"Processed PDF: {file_path.name}")
+
+            elif file_path.suffix.lower() == '.json':
+                with open(file_path, 'r', encoding='utf-8') as f:
+                    data = json.load(f)
+                    # Ensure data is a list of documents
+                    docs = data if isinstance(data, list) else [data]
+                logging.info(f"Processed JSON: {file_path.name}")
+            
+            else:
+                logging.warning(f"Unsupported file type: {file_path.name}")
+                return None
+            
+            return docs
+
+        except Exception as e:
+            logging.error(f"Failed to process file {file_path_str}: {e}")
+            return None
+
+def main():
+    processor = DocumentProcessor()
+    connection = connect_to_rabbitmq()
+    channel = connection.channel()
+
+    # Declare queues
+    channel.queue_declare(queue=CONSUME_QUEUE, durable=True)
+    channel.queue_declare(queue=PUBLISH_QUEUE, durable=True)
+
+    def callback(ch, method, properties, body):
+        try:
+            message = json.loads(body)
+            file_path = message.get('file_path')
+            if file_path:
+                logging.info(f"Received message to process file: {file_path}")
+                processed_docs = processor.process_file(file_path)
+                
+                if processed_docs:
+                    # Publish processed documents to the next queue
+                    channel.basic_publish(
+                        exchange='',
+                        routing_key=PUBLISH_QUEUE,
+                        body=json.dumps(processed_docs),
+                        properties=pika.BasicProperties(
+                            delivery_mode=2,  # make message persistent
+                        ))
+                    logging.info(f"Published {len(processed_docs)} processed documents to '{PUBLISH_QUEUE}' queue.")
+            
+            ch.basic_ack(delivery_tag=method.delivery_tag)
+
+        except json.JSONDecodeError:
+            logging.error("Failed to decode message body.")
+            ch.basic_ack(delivery_tag=method.delivery_tag)
+        except Exception as e:
+            logging.error(f"An error occurred in callback: {e}")
+            # Decide if you want to requeue or not
+            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
+
+
+    channel.basic_qos(prefetch_count=1)
+    channel.basic_consume(queue=CONSUME_QUEUE, on_message_callback=callback)
+
+    logging.info(f"Waiting for messages on '{CONSUME_QUEUE}'. To exit press CTRL+C")
+    try:
+        channel.start_consuming()
+    except KeyboardInterrupt:
+        logging.info("Stopping consumer.")
+        channel.stop_consuming()
+    finally:
+        connection.close()
+        logging.info("RabbitMQ connection closed.")
+
+if __name__ == "__main__":
+    main()
diff --git a/docker-compose.yml b/docker-compose.yml
index ca91793..9f56ba7 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -18,41 +18,93 @@ services:
       timeout: 10s
       retries: 3
 
-  json-ingester:
+  rabbitmq:
+    image: rabbitmq:3-management
+    container_name: rabbitmq
+    ports:
+      - "5672:5672"
+      - "15672:15672"
+    volumes:
+      - ./data/rabbitmq:/var/lib/rabbitmq
+    environment:
+      - RABBITMQ_DEFAULT_USER=user
+      - RABBITMQ_DEFAULT_PASS=password
+    healthcheck:
+      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
+      interval: 30s
+      timeout: 10s
+      retries: 3
+
+  file-watcher:
     build:
       context: .
-      dockerfile: Dockerfile.ingester
-    container_name: json-ingester
+      dockerfile: Dockerfile.file_watcher
+    container_name: file-watcher
     volumes:
-      - ./input/json:/input/json:ro
+      - ./input:/input:ro
       - ./logs:/logs
     environment:
-      - MEILISEARCH_URL=http://meilisearch:7700
-      - MEILISEARCH_API_KEY=${MEILI_MASTER_KEY:-your_secure_master_key_123}
-      - INDEX_NAME=documents
-      - MODE=json
-      - INPUT_DIR=/input/json
-      - LOG_FILE_PATH=/logs/json-ingester.log
+      - RABBITMQ_HOST=rabbitmq
+      - RABBITMQ_USER=user
+      - RABBITMQ_PASS=password
+      - RABBITMQ_QUEUE=file_events
+      - WATCH_DIRECTORY=/input
     depends_on:
-      meilisearch:
+      rabbitmq:
         condition: service_healthy
     restart: unless-stopped
 
-  pdf-ingester:
+  doc-processor:
     build:
       context: .
-      dockerfile: Dockerfile.ingester
-    container_name: pdf-ingester
+      dockerfile: Dockerfile.doc_processor
+    container_name: doc-processor
+    volumes:
+      - ./input:/input:ro
+      - ./logs:/logs
+    environment:
+      - RABBITMQ_HOST=rabbitmq
+      - RABBITMQ_USER=user
+      - RABBITMQ_PASS=password
+      - CONSUME_QUEUE=file_events
+      - PUBLISH_QUEUE=processed_docs
+    depends_on:
+      rabbitmq:
+        condition: service_healthy
+    restart: unless-stopped
+
+  meili-ingester:
+    build:
+      context: .
+      dockerfile: Dockerfile.meili_ingester
+    container_name: meili-ingester
     volumes:
-      - ./input/pdf:/input/pdf:ro
       - ./logs:/logs
     environment:
       - MEILISEARCH_URL=http://meilisearch:7700
-      - MEILISEARCH_API_KEY=${MEILI_MASTER_KEY:-your_secure_master_key_123}
+      - MEILISEARCH_API_KEY=${MEILI_MASTER_KEY}
       - INDEX_NAME=documents
-      - MODE=pdf
-      - INPUT_DIR=/input/pdf
-      - LOG_FILE_PATH=/logs/pdf-ingester.log
+      - RABBITMQ_HOST=rabbitmq
+      - RABBITMQ_USER=user
+      - RABBITMQ_PASS=password
+      - CONSUME_QUEUE=processed_docs
+    depends_on:
+      rabbitmq:
+        condition: service_healthy
+      meilisearch:
+        condition: service_healthy
+    restart: unless-stopped
+
+  fastmcp:
+    build:
+      context: .
+      dockerfile: Dockerfile.fastmcp
+    container_name: fastmcp
+    ports:
+      - "8000:8000"
+    environment:
+      - MEILISEARCH_URL=http://meilisearch:7700
+      - MEILISEARCH_API_KEY=${MEILI_MASTER_KEY}
     depends_on:
       meilisearch:
         condition: service_healthy
diff --git a/fastmcp/main.py b/fastmcp/main.py
new file mode 100644
index 0000000..88e6081
--- /dev/null
+++ b/fastmcp/main.py
@@ -0,0 +1,77 @@
+
+import os
+from fastapi import FastAPI, HTTPException, Request, Response
+from pydantic import BaseModel
+from starlette.middleware.base import BaseHTTPMiddleware
+from meilisearch import Client
+from meilisearch.errors import MeilisearchApiError
+
+# --- Configuration ---
+MEILISEARCH_URL = os.getenv("MEILISEARCH_URL", "http://localhost:7700")
+MEILISEARCH_API_KEY = os.getenv("MEILISEARCH_API_KEY")
+INDEX_NAME = os.getenv("INDEX_NAME", "documents")
+AUTH_ENABLED = os.getenv("AUTH_ENABLED", "true").lower() == "true"
+DUMMY_AUTH_TOKEN = os.getenv("DUMMY_AUTH_TOKEN", "DUMMY_SECRET_TOKEN")
+
+# --- Meilisearch Client ---
+client = Client(url=MEILISEARCH_URL, api_key=MEILISEARCH_API_KEY)
+
+# --- FastAPI App ---
+app = FastAPI(
+    title="FastMCP",
+    description="A secure and fast gateway for Meilisearch",
+    version="0.1.0",
+)
+
+# --- Authentication Middleware ---
+class UserAuthMiddleware(BaseHTTPMiddleware):
+    async def dispatch(self, request: Request, call_next):
+        if not AUTH_ENABLED or request.url.path == "/health":
+            response = await call_next(request)
+            return response
+
+        auth_header = request.headers.get("Authorization")
+        if not auth_header:
+            return Response("Authorization header is missing", status_code=401)
+
+        try:
+            scheme, token = auth_header.split()
+            if scheme.lower() != "bearer" or token != DUMMY_AUTH_TOKEN:
+                return Response("Invalid authentication credentials", status_code=401)
+        except ValueError:
+            return Response("Invalid authorization header format", status_code=401)
+
+        # You could attach user info to the request state if needed
+        # request.state.user_id = "dummy_user"
+        
+        response = await call_next(request)
+        return response
+
+app.add_middleware(UserAuthMiddleware)
+
+
+# --- Pydantic Models ---
+class SearchRequest(BaseModel):
+    query: str
+    index_name: str = INDEX_NAME
+    # Add other Meilisearch parameters as needed, e.g., limit, offset, filter
+
+# --- API Endpoints ---
+@app.get("/health")
+def health_check():
+    """Simple health check endpoint."""
+    return {"status": "ok"}
+
+@app.post("/search")
+def search(request: SearchRequest):
+    """
+    Forwards a search query to the Meilisearch instance.
+    """
+    try:
+        index = client.index(request.index_name)
+        search_results = index.search(request.query)
+        return search_results
+    except MeilisearchApiError as e:
+        raise HTTPException(status_code=e.status_code, detail=e.message)
+    except Exception as e:
+        raise HTTPException(status_code=500, detail=f"An internal error occurred: {e}")
diff --git a/file_watcher.py b/file_watcher.py
new file mode 100644
index 0000000..426e3ab
--- /dev/null
+++ b/file_watcher.py
@@ -0,0 +1,82 @@
+
+import os
+import time
+import logging
+import json
+import pika
+from pika.exceptions import AMQPConnectionError
+from watchdog.observers import Observer
+from watchdog.events import FileSystemEventHandler
+
+# Logging configuration
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+
+# Environment variables
+RABBITMQ_HOST = os.getenv('RABBITMQ_HOST', 'rabbitmq')
+RABBITMQ_USER = os.getenv('RABBITMQ_USER', 'user')
+RABBITMQ_PASS = os.getenv('RABBITMQ_PASS', 'password')
+QUEUE_NAME = os.getenv('RABBITMQ_QUEUE', 'file_events')
+WATCH_DIRECTORY = os.getenv('WATCH_DIRECTORY', '/input')
+
+def connect_to_rabbitmq():
+    """Establish a connection to RabbitMQ, with retries."""
+    credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASS)
+    parameters = pika.ConnectionParameters(RABBITMQ_HOST, credentials=credentials)
+    while True:
+        try:
+            connection = pika.BlockingConnection(parameters)
+            logging.info("Successfully connected to RabbitMQ.")
+            return connection
+        except AMQPConnectionError as e:
+            logging.error(f"Failed to connect to RabbitMQ: {e}. Retrying in 5 seconds...")
+            time.sleep(5)
+
+class FileChangeHandler(FileSystemEventHandler):
+    def __init__(self, channel, queue_name):
+        self.channel = channel
+        self.queue_name = queue_name
+
+    def on_created(self, event):
+        if not event.is_directory:
+            self.publish_message(event.src_path)
+
+    def publish_message(self, file_path):
+        """Publish a message to the RabbitMQ queue."""
+        try:
+            message = json.dumps({'file_path': file_path})
+            self.channel.basic_publish(
+                exchange='',
+                routing_key=self.queue_name,
+                body=message,
+                properties=pika.BasicProperties(
+                    delivery_mode=2,  # make message persistent
+                ))
+            logging.info(f"Sent message for new file: {file_path}")
+        except Exception as e:
+            logging.error(f"Failed to send message for {file_path}: {e}")
+
+def main():
+    connection = connect_to_rabbitmq()
+    channel = connection.channel()
+    channel.queue_declare(queue=QUEUE_NAME, durable=True)
+
+    event_handler = FileChangeHandler(channel, QUEUE_NAME)
+    observer = Observer()
+    observer.schedule(event_handler, WATCH_DIRECTORY, recursive=True)
+    observer.start()
+
+    logging.info(f"Watching directory: {WATCH_DIRECTORY}")
+
+    try:
+        while True:
+            time.sleep(1)
+    except KeyboardInterrupt:
+        observer.stop()
+        logging.info("Observer stopped.")
+    finally:
+        observer.join()
+        connection.close()
+        logging.info("RabbitMQ connection closed.")
+
+if __name__ == "__main__":
+    main()
diff --git a/ingester.py b/ingester.py
deleted file mode 100644
index 2f15b01..0000000
--- a/ingester.py
+++ /dev/null
@@ -1,108 +0,0 @@
-import os
-import json
-import time
-import logging
-import sys
-from pathlib import Path
-from watchdog.observers import Observer
-from watchdog.events import FileSystemEventHandler
-from meilisearch import Client
-from docling.document_converter import DocumentConverter
-
-# 環境変数からログファイルパスを取得、未設定の場合は標準出力のみ
-log_file_path = os.getenv('LOG_FILE_PATH')
-
-# ロガーの基本設定
-log_format = '%(asctime)s - %(levelname)s - %(message)s'
-log_level = logging.INFO
-handlers = [logging.StreamHandler()]
-
-# ログファイルパスが指定されていれば、ファイルハンドラも追加
-if log_file_path:
-    # ログディレクトリが存在しない場合は作成
-    log_dir = os.path.dirname(log_file_path)
-    if not os.path.exists(log_dir):
-        os.makedirs(log_dir)
-    handlers.append(logging.FileHandler(log_file_path, encoding='utf-8'))
-
-logging.basicConfig(
-    level=log_level,
-    format=log_format,
-    handlers=handlers
-)
-
-
-class IngesterHandler(FileSystemEventHandler):
-    def __init__(self, client, index_name, input_dir, mode):
-        self.client = client
-        self.index_name = index_name
-        self.input_dir = Path(input_dir)
-        self.mode = mode  # 'json' or 'pdf'
-        self.converter = DocumentConverter() if mode == 'pdf' else None
-
-    def on_created(self, event):
-        if event.is_directory:
-            return
-        ext = '.json' if self.mode == 'json' else '.pdf'
-        if event.src_path.lower().endswith(ext):
-            time.sleep(1)  # ファイル書き込み完了待機
-            self.process_file(event.src_path)
-
-    def process_file(self, file_path):
-        try:
-            docs = []
-            path = Path(file_path)
-            if self.mode == 'pdf':
-                # Docling で高度抽出（Heronモデル）
-                result = self.converter.convert(file_path)
-                markdown = result.document.export_to_markdown()
-                doc = {
-                    "id": path.stem,
-                    "content": markdown,
-                    "type": "pdf",
-                    "source": path.name,
-                    "metadata": {
-                        "format": "markdown",
-                        "page_count": len(result.document.pages) if hasattr(result.document, 'pages') else 0
-                    }
-                }
-                docs.append(doc)
-                logging.info(f"PDF → Markdown: {path.name} ({len(markdown)}文字)")
-            else:
-                # JSON 投入
-                with open(file_path, 'r', encoding='utf-8') as f:
-                    data = json.load(f)
-                    docs = data if isinstance(data, list) else [data]
-
-            # Meilisearch に投入
-            task = self.client.index(self.index_name).add_documents(docs)
-            logging.info(f"投入成功: {len(docs)}件 → index={self.index_name}, task={task.task_uid}")
-        except Exception as e:
-            logging.error(f"処理失敗 {file_path}: {e}")
-
-def main():
-    url = os.getenv('MEILISEARCH_URL', 'http://localhost:7700')
-    api_key = os.getenv('MEILISEARCH_API_KEY')
-    index_name = os.getenv('INDEX_NAME', 'documents')
-    mode = os.getenv('MODE', 'json').lower()
-
-    # 環境変数から入力ディレクトリを取得
-    input_dir = os.getenv('INPUT_DIR', '/input/json') if mode == 'json' else os.getenv('INPUT_DIR', '/input/pdf')
-
-    client = Client(url, api_key)
-    handler = IngesterHandler(client, index_name, input_dir, mode)
-
-    observer = Observer()
-    observer.schedule(handler, input_dir, recursive=False)
-    observer.start()
-
-    logging.info(f"{mode.upper()} Ingester 起動 → {input_dir} → index: {index_name}")
-    try:
-        while True:
-            time.sleep(1)
-    except KeyboardInterrupt:
-        observer.stop()
-    observer.join()
-
-if __name__ == '__main__':
-    main()
diff --git a/meili_ingester.py b/meili_ingester.py
new file mode 100644
index 0000000..37c58cb
--- /dev/null
+++ b/meili_ingester.py
@@ -0,0 +1,91 @@
+
+import os
+import json
+import time
+import logging
+import pika
+from pika.exceptions import AMQPConnectionError
+from meilisearch import Client
+from meilisearch.errors import MeilisearchApiError
+
+# Logging configuration
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+
+# Environment variables from docker-compose
+MEILISEARCH_URL = os.getenv('MEILISEARCH_URL', 'http://meilisearch:7700')
+MEILISEARCH_API_KEY = os.getenv('MEILISEARCH_API_KEY')
+INDEX_NAME = os.getenv('INDEX_NAME', 'documents')
+RABBITMQ_HOST = os.getenv('RABBITMQ_HOST', 'rabbitmq')
+RABBITMQ_USER = os.getenv('RABBITMQ_USER', 'user')
+RABBITMQ_PASS = os.getenv('RABBITMQ_PASS', 'password')
+CONSUME_QUEUE = os.getenv('CONSUME_QUEUE', 'processed_docs')
+
+def connect_to_rabbitmq():
+    """Establish a connection to RabbitMQ, with retries."""
+    credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASS)
+    parameters = pika.ConnectionParameters(RABBITMQ_HOST, credentials=credentials)
+    while True:
+        try:
+            connection = pika.BlockingConnection(parameters)
+            logging.info("Successfully connected to RabbitMQ.")
+            return connection
+        except AMQPConnectionError as e:
+            logging.error(f"Failed to connect to RabbitMQ: {e}. Retrying in 5 seconds...")
+            time.sleep(5)
+
+def get_meilisearch_client():
+    """Get a Meilisearch client instance."""
+    return Client(MEILISEARCH_URL, MEILISEARCH_API_KEY)
+
+def main():
+    meili_client = get_meilisearch_client()
+    connection = connect_to_rabbitmq()
+    channel = connection.channel()
+
+    channel.queue_declare(queue=CONSUME_QUEUE, durable=True)
+
+    def callback(ch, method, properties, body):
+        try:
+            documents = json.loads(body)
+            if not documents:
+                logging.warning("Received an empty list of documents.")
+                ch.basic_ack(delivery_tag=method.delivery_tag)
+                return
+
+            logging.info(f"Received {len(documents)} documents to ingest.")
+            
+            try:
+                task = meili_client.index(INDEX_NAME).add_documents(documents)
+                logging.info(f"Successfully sent {len(documents)} documents to Meilisearch. Task UID: {task.task_uid}")
+            except MeilisearchApiError as e:
+                logging.error(f"Failed to ingest documents into Meilisearch: {e}")
+                # Depending on the error, you might want to requeue
+                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
+                return
+
+            ch.basic_ack(delivery_tag=method.delivery_tag)
+
+        except json.JSONDecodeError:
+            logging.error("Failed to decode message body.")
+            ch.basic_ack(delivery_tag=method.delivery_tag)
+        except Exception as e:
+            logging.error(f"An unexpected error occurred in callback: {e}")
+            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
+
+
+    channel.basic_qos(prefetch_count=1)
+    channel.basic_consume(queue=CONSUME_QUEUE, on_message_callback=callback)
+
+    logging.info(f"Waiting for processed documents on '{CONSUME_QUEUE}'. To exit press CTRL+C")
+    try:
+        channel.start_consuming()
+    except KeyboardInterrupt:
+        logging.info("Stopping consumer.")
+        channel.stop_consuming()
+    finally:
+        connection.close()
+        logging.info("RabbitMQ connection closed.")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/requirements.txt b/requirements.txt
index 8f65999..79791af 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,5 +1,8 @@
 meilisearch==0.37.1
 watchdog==4.0.0
+pika==1.3.2
 docling>=2.0.0
+fastapi==0.111.0
+uvicorn==0.29.0
 pytest==8.2.2
 pytest-mock==3.14.0

